RNN / Embed / Sent = <class 'keras.layers.recurrent.LSTM'>, 300, 300
GloVe / Trainable Word Embeddings = False, True
Build model...
Vocab size = 11622
Model: "model_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 42)           0                                            
__________________________________________________________________________________________________
input_2 (InputLayer)            (None, 42)           0                                            
__________________________________________________________________________________________________
embedding_1 (Embedding)         (None, 42, 300)      3486600     input_1[0][0]                    
                                                                 input_2[0][0]                    
__________________________________________________________________________________________________
time_distributed_1 (TimeDistrib (None, 42, 300)      90300       embedding_1[0][0]                
                                                                 embedding_1[1][0]                
__________________________________________________________________________________________________
lstm_1 (LSTM)                   (None, 42, 300)      721200      time_distributed_1[0][0]         
                                                                 time_distributed_1[1][0]         
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 42, 300)      1200        lstm_1[0][0]                     
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 42, 300)      1200        lstm_1[1][0]                     
__________________________________________________________________________________________________
lstm_2 (LSTM)                   (None, 42, 300)      721200      batch_normalization_1[0][0]      
                                                                 batch_normalization_2[0][0]      
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 42, 300)      1200        lstm_2[0][0]                     
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 42, 300)      1200        lstm_2[1][0]                     
__________________________________________________________________________________________________
lstm_3 (LSTM)                   (None, 300)          721200      batch_normalization_3[0][0]      
                                                                 batch_normalization_4[0][0]      
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 300)          1200        lstm_3[0][0]                     
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 300)          1200        lstm_3[1][0]                     
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 600)          0           batch_normalization_5[0][0]      
                                                                 batch_normalization_6[0][0]      
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 600)          0           concatenate_1[0][0]              
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 600)          360600      dropout_1[0][0]                  
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 600)          0           dense_2[0][0]                    
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 600)          2400        dropout_2[0][0]                  
__________________________________________________________________________________________________
dense_3 (Dense)                 (None, 600)          360600      batch_normalization_7[0][0]      
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 600)          0           dense_3[0][0]                    
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 600)          2400        dropout_3[0][0]                  
__________________________________________________________________________________________________
dense_4 (Dense)                 (None, 600)          360600      batch_normalization_8[0][0]      
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 600)          0           dense_4[0][0]                    
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 600)          2400        dropout_4[0][0]                  
__________________________________________________________________________________________________
dense_5 (Dense)                 (None, 600)          360600      batch_normalization_9[0][0]      
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 600)          0           dense_5[0][0]                    
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 600)          2400        dropout_5[0][0]                  
__________________________________________________________________________________________________
dense_6 (Dense)                 (None, 4)            2404        batch_normalization_10[0][0]     
==================================================================================================
Total params: 7,202,104
Trainable params: 7,193,704
Non-trainable params: 8,400
__________________________________________________________________________________________________
Training
Train on 2692 samples, validate on 769 samples
Epoch 1/42

 512/2692 [====>.........................] - ETA: 40s - loss: 1.9964 - accuracy: 0.2383
1024/2692 [==========>...................] - ETA: 26s - loss: 1.9423 - accuracy: 0.2461
1536/2692 [================>.............] - ETA: 17s - loss: 1.9452 - accuracy: 0.2552
2048/2692 [=====================>........] - ETA: 9s - loss: 1.9261 - accuracy: 0.2661 
2560/2692 [===========================>..] - ETA: 1s - loss: 1.8882 - accuracy: 0.2730
2692/2692 [==============================] - 42s 16ms/step - loss: 1.8808 - accuracy: 0.2753 - val_loss: 1.2693 - val_accuracy: 0.5761
Epoch 2/42

 512/2692 [====>.........................] - ETA: 29s - loss: 1.7572 - accuracy: 0.3438
1024/2692 [==========>...................] - ETA: 22s - loss: 1.7503 - accuracy: 0.3311
1536/2692 [================>.............] - ETA: 15s - loss: 1.7003 - accuracy: 0.3281
2048/2692 [=====================>........] - ETA: 8s - loss: 1.6857 - accuracy: 0.3257 
2560/2692 [===========================>..] - ETA: 1s - loss: 1.6583 - accuracy: 0.3297
2692/2692 [==============================] - 39s 14ms/step - loss: 1.6534 - accuracy: 0.3302 - val_loss: 1.2264 - val_accuracy: 0.5761
Epoch 3/42

 512/2692 [====>.........................] - ETA: 33s - loss: 1.5756 - accuracy: 0.3398
1024/2692 [==========>...................] - ETA: 24s - loss: 1.6036 - accuracy: 0.3477
1536/2692 [================>.............] - ETA: 16s - loss: 1.5683 - accuracy: 0.3600
2048/2692 [=====================>........] - ETA: 8s - loss: 1.5502 - accuracy: 0.3594 
2560/2692 [===========================>..] - ETA: 1s - loss: 1.5553 - accuracy: 0.3566
2692/2692 [==============================] - 40s 15ms/step - loss: 1.5506 - accuracy: 0.3581 - val_loss: 1.1818 - val_accuracy: 0.5761
Epoch 4/42

 512/2692 [====>.........................] - ETA: 28s - loss: 1.5273 - accuracy: 0.3594
1024/2692 [==========>...................] - ETA: 24s - loss: 1.4733 - accuracy: 0.3828
1536/2692 [================>.............] - ETA: 17s - loss: 1.4798 - accuracy: 0.3776
2048/2692 [=====================>........] - ETA: 9s - loss: 1.4660 - accuracy: 0.3809 
2560/2692 [===========================>..] - ETA: 2s - loss: 1.4666 - accuracy: 0.3852
2692/2692 [==============================] - 45s 17ms/step - loss: 1.4609 - accuracy: 0.3871 - val_loss: 1.1323 - val_accuracy: 0.5761
Epoch 5/42

 512/2692 [====>.........................] - ETA: 37s - loss: 1.4826 - accuracy: 0.3867
1024/2692 [==========>...................] - ETA: 26s - loss: 1.4763 - accuracy: 0.3906
1536/2692 [================>.............] - ETA: 35s - loss: 1.4741 - accuracy: 0.3900
2048/2692 [=====================>........] - ETA: 33s - loss: 1.4373 - accuracy: 0.4048
2560/2692 [===========================>..] - ETA: 6s - loss: 1.4294 - accuracy: 0.4066 
2692/2692 [==============================] - 168s 62ms/step - loss: 1.4272 - accuracy: 0.4079 - val_loss: 1.1045 - val_accuracy: 0.5761
Epoch 6/42

 512/2692 [====>.........................] - ETA: 55s - loss: 1.3803 - accuracy: 0.4609
1024/2692 [==========>...................] - ETA: 33s - loss: 1.3665 - accuracy: 0.4424
1536/2692 [================>.............] - ETA: 22s - loss: 1.3812 - accuracy: 0.4251
2048/2692 [=====================>........] - ETA: 13s - loss: 1.3676 - accuracy: 0.4282
2560/2692 [===========================>..] - ETA: 3s - loss: 1.3650 - accuracy: 0.4281 
2692/2692 [==============================] - 74s 27ms/step - loss: 1.3640 - accuracy: 0.4298 - val_loss: 1.1087 - val_accuracy: 0.5761
Epoch 7/42

 512/2692 [====>.........................] - ETA: 1:10 - loss: 1.3533 - accuracy: 0.4492
1024/2692 [==========>...................] - ETA: 46s - loss: 1.3651 - accuracy: 0.4385 
1536/2692 [================>.............] - ETA: 38s - loss: 1.3521 - accuracy: 0.4486
2048/2692 [=====================>........] - ETA: 20s - loss: 1.3560 - accuracy: 0.4492
2560/2692 [===========================>..] - ETA: 4s - loss: 1.3474 - accuracy: 0.4473 
2692/2692 [==============================] - 98s 37ms/step - loss: 1.3453 - accuracy: 0.4476 - val_loss: 1.0867 - val_accuracy: 0.5761
Epoch 8/42

 512/2692 [====>.........................] - ETA: 1:09 - loss: 1.3624 - accuracy: 0.4336
1024/2692 [==========>...................] - ETA: 51s - loss: 1.2948 - accuracy: 0.4707 
1536/2692 [================>.............] - ETA: 33s - loss: 1.2909 - accuracy: 0.4766
2048/2692 [=====================>........] - ETA: 19s - loss: 1.2838 - accuracy: 0.4829
2560/2692 [===========================>..] - ETA: 4s - loss: 1.2805 - accuracy: 0.4809 
2692/2692 [==============================] - 90s 33ms/step - loss: 1.2775 - accuracy: 0.4825 - val_loss: 1.0778 - val_accuracy: 0.5761
Epoch 9/42

 512/2692 [====>.........................] - ETA: 1:05 - loss: 1.3097 - accuracy: 0.4746
1024/2692 [==========>...................] - ETA: 51s - loss: 1.2996 - accuracy: 0.4746 
1536/2692 [================>.............] - ETA: 35s - loss: 1.2959 - accuracy: 0.4870
2048/2692 [=====================>........] - ETA: 19s - loss: 1.2808 - accuracy: 0.4878
2560/2692 [===========================>..] - ETA: 3s - loss: 1.2750 - accuracy: 0.4906 
2692/2692 [==============================] - 89s 33ms/step - loss: 1.2740 - accuracy: 0.4911 - val_loss: 1.0687 - val_accuracy: 0.5761
Epoch 10/42

 512/2692 [====>.........................] - ETA: 1:09 - loss: 1.3256 - accuracy: 0.4727
1024/2692 [==========>...................] - ETA: 51s - loss: 1.3008 - accuracy: 0.4844 
1536/2692 [================>.............] - ETA: 34s - loss: 1.2850 - accuracy: 0.4954
2048/2692 [=====================>........] - ETA: 20s - loss: 1.2663 - accuracy: 0.4951
2560/2692 [===========================>..] - ETA: 4s - loss: 1.2546 - accuracy: 0.5004 
2692/2692 [==============================] - 94s 35ms/step - loss: 1.2561 - accuracy: 0.4989 - val_loss: 1.0584 - val_accuracy: 0.5761
Epoch 11/42

 512/2692 [====>.........................] - ETA: 1:09 - loss: 1.1957 - accuracy: 0.5254
1024/2692 [==========>...................] - ETA: 52s - loss: 1.2244 - accuracy: 0.5166 
1536/2692 [================>.............] - ETA: 34s - loss: 1.2172 - accuracy: 0.5117
2048/2692 [=====================>........] - ETA: 19s - loss: 1.2282 - accuracy: 0.5059
2560/2692 [===========================>..] - ETA: 3s - loss: 1.2275 - accuracy: 0.5008 
2692/2692 [==============================] - 90s 34ms/step - loss: 1.2273 - accuracy: 0.5026 - val_loss: 1.0703 - val_accuracy: 0.5761
Epoch 12/42

 512/2692 [====>.........................] - ETA: 1:04 - loss: 1.3094 - accuracy: 0.4941
1024/2692 [==========>...................] - ETA: 47s - loss: 1.2776 - accuracy: 0.4932 
1536/2692 [================>.............] - ETA: 31s - loss: 1.2578 - accuracy: 0.4954
2048/2692 [=====================>........] - ETA: 18s - loss: 1.2439 - accuracy: 0.5059
2560/2692 [===========================>..] - ETA: 3s - loss: 1.2460 - accuracy: 0.5035 
2692/2692 [==============================] - 86s 32ms/step - loss: 1.2395 - accuracy: 0.5048 - val_loss: 1.0685 - val_accuracy: 0.5761
Epoch 13/42

 512/2692 [====>.........................] - ETA: 1:03 - loss: 1.1975 - accuracy: 0.5137
1024/2692 [==========>...................] - ETA: 50s - loss: 1.2050 - accuracy: 0.5215 
1536/2692 [================>.............] - ETA: 32s - loss: 1.1936 - accuracy: 0.5319
2048/2692 [=====================>........] - ETA: 18s - loss: 1.2023 - accuracy: 0.5234
2560/2692 [===========================>..] - ETA: 3s - loss: 1.1932 - accuracy: 0.5250 
2692/2692 [==============================] - 85s 32ms/step - loss: 1.1943 - accuracy: 0.5279 - val_loss: 1.0708 - val_accuracy: 0.5761
Epoch 14/42

 512/2692 [====>.........................] - ETA: 1:07 - loss: 1.2924 - accuracy: 0.4824
1024/2692 [==========>...................] - ETA: 50s - loss: 1.2386 - accuracy: 0.4980 
1536/2692 [================>.............] - ETA: 32s - loss: 1.2035 - accuracy: 0.5124
2048/2692 [=====================>........] - ETA: 18s - loss: 1.2016 - accuracy: 0.5186
2560/2692 [===========================>..] - ETA: 3s - loss: 1.1977 - accuracy: 0.5180 
2692/2692 [==============================] - 85s 32ms/step - loss: 1.1900 - accuracy: 0.5193 - val_loss: 1.0762 - val_accuracy: 0.5761

385/385 [==============================] - 4s 10ms/step
Test loss / test accuracy = 1.0219 / 0.6026
